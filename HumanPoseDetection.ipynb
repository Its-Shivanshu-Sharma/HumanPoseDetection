{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lYUOPy2fl4ix"
   },
   "source": [
    "<div align=\"right\">\n",
    "    <a href=\"https://colab.research.google.com/github/Its-Shivanshu-Sharma/HumanPoseDetection/blob/main/HumanPoseDetection.ipynb\">\n",
    "        <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open in Colab\"/>\n",
    "    </a>\n",
    "    <br/>\n",
    "    <a href=\"https://console.paperspace.com/github/Its-Shivanshu-Sharma/HumanPoseDetection/blob/main/HumanPoseDetection.ipynb\">\n",
    "        <img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"/>\n",
    "    </a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J5H1r-t0n8et"
   },
   "source": [
    "## Installing & Importing libraries, packages, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_U4BlQltnrGN",
    "outputId": "8cdfd341-dea8-44b3-f7ad-a6b8a430d6ad"
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "pip3 install -qq torch==1.10.2+cu113 torchvision==0.11.3+cu113 -f https://download.pytorch.org/whl/cu113/torch_stable.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kbTjTbjZoLBp"
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "import os\n",
    "import pathlib\n",
    "import time\n",
    "\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as T\n",
    "from google.colab.patches import cv2_imshow\n",
    "from scipy.io import loadmat\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aRSMwWYxoxoC"
   },
   "source": [
    "## Fetching & Preparing the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4bjdiMK4z9nn",
    "outputId": "4749cbfd-b186-44b1-f2d0-8e1f70aaf832"
   },
   "outputs": [],
   "source": [
    "# Set the seed for the RNG(Random Number Generator manually\n",
    "# (for reproducibility)\n",
    "torch.manual_seed(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZMxLRWJm2fDn"
   },
   "source": [
    "### NOTE:\n",
    "The dataset used in this project is the [`Leeds Sports Pose Dataset`](http://sam.johnson.io/research/lsp.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u5hNxqfCHsSR"
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Create a new directory called `pose_dataset` to store the data\n",
    "mkdir pose_dataset\n",
    "cd pose_dataset\n",
    "wget -q http://sam.johnson.io/research/lsp_dataset.zip\n",
    "unzip -qq lsp_dataset.zip\n",
    "rm lsp_dataset.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "38Tc_eQzpkmV"
   },
   "outputs": [],
   "source": [
    "root = pathlib.Path(\"./pose_dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FJJ-qevn8xp2"
   },
   "source": [
    "#### The annotations for the joints for each image are in a `joints.mat` file, hence, we make use of the `scipy.io.loadmat` function to read this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J5094mpzKb6l"
   },
   "outputs": [],
   "source": [
    "raw_annotations = loadmat(root / \"joints.mat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3uYwwN7_BPZL"
   },
   "outputs": [],
   "source": [
    "raw_annotations_x = raw_annotations[\"joints\"][0].T.reshape(-1, 1).squeeze()\n",
    "raw_annotations_y = raw_annotations[\"joints\"][1].T.reshape(-1, 1).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A37N2CTGuwOt",
    "outputId": "c970514f-5a35-4555-d3b2-73b3062475ed"
   },
   "outputs": [],
   "source": [
    "raw_annot_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LrB9eHWHY_As"
   },
   "outputs": [],
   "source": [
    "# No. of joints annotated for each image\n",
    "n_joints = 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "njahaw_U8guV"
   },
   "outputs": [],
   "source": [
    "index = pd.MultiIndex.from_product(\n",
    "    (range(len(raw_annotations) // n_joints), range(n_joints)),\n",
    "    names=[\"image_num\", \"joint\"],\n",
    ")\n",
    "cols = pd.Index([\"x\", \"y\"], name=\"coordinates\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 455
    },
    "id": "c5tDYfvfKr-C",
    "outputId": "ee4153ef-2b02-405a-bd7f-f82dd7180b42"
   },
   "outputs": [],
   "source": [
    "full_annotations = pd.DataFrame(\n",
    "    zip(raw_annotations_x, raw_annotations_y), index=index, columns=cols\n",
    ")\n",
    "full_annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 550
    },
    "id": "ANBWo9M5lA-F",
    "outputId": "4d75f368-ca6e-4210-a95a-b7fa91b24501"
   },
   "outputs": [],
   "source": [
    "full_annotations = annot.unstack(1).swaplevel(i=0, j=1, axis=1).sort_index(axis=1)\n",
    "full_annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dYG2cb1saVdx"
   },
   "outputs": [],
   "source": [
    "def train_val_split(annotations, train_size=None, val_size=None):\n",
    "    \"\"\"Function for randomly splitting the annotations into training &\n",
    "    validation sets as  per the size specified by the parmaters `train_size`\n",
    "    or `val_size`.\n",
    "    Indices of training & validation sets are reset to 0. The old indices\n",
    "    (which specify the image) are stored in a new column named `image_num`.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    - annotations (pandas.DataFrame): Dataframe containing the annotations.\n",
    "    - train_size (float): Fraction of the data to be allocated to training set.\n",
    "    - val_size (float): Fraction of the data to be allocated to validation set.\n",
    "                        This parameter is ignored if `train_size` is not None.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    Tuple containing the training & validation sets,\n",
    "    i.e. `(train_annotations, val_annotations)`.\n",
    "\n",
    "    \"\"\"\n",
    "    # Calculate length of training set\n",
    "    annotation_length = len(annotations)\n",
    "    if train_size:\n",
    "        train_len = int(annotation_length * train_size)\n",
    "    elif val_size:\n",
    "        train_len = annotation_length - int(annotation_length * val_size)\n",
    "\n",
    "    # Generate random indices\n",
    "    rand_idxs = torch.randperm(annotation_length)\n",
    "    train_annotations = annotations.loc[rand_idxs[:train_len]]\n",
    "    val_annotations = annotations.loc[rand_idxs[train_len:]]\n",
    "\n",
    "    # Reset indices of `train_annotations` & `val_annotations` to start from 0\n",
    "    # (& add `image_num` column)\n",
    "    train_annotations = train_annotations.reset_index().sort_index(axis=1)\n",
    "    val_annotations = val_annotations.reset_index().sort_index(axis=1)\n",
    "\n",
    "    return train_annotations, val_annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Hhr7sisgb84L",
    "outputId": "d72a90cb-2af5-41f0-b53b-bdb76778296f"
   },
   "outputs": [],
   "source": [
    "annotations = {}\n",
    "annotations[\"train\"], annotations[\"val\"] = train_val_split(\n",
    "    full_annotations, train_size=0.8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_k7mpQwdcrYF"
   },
   "outputs": [],
   "source": [
    "for phase in [\"train\", \"val\"]:\n",
    "    print(f\"Size of {phase} set: {len(annotations[phase])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EXroCmDXk97k"
   },
   "source": [
    "### Creating `Dataset` & `DataLoader` objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bcHHTaDftak0"
   },
   "outputs": [],
   "source": [
    "class PoseDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        annotations,\n",
    "        img_dir,\n",
    "        img_format=\"jpg\",\n",
    "        transform=None,\n",
    "        target_transform=None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        -----------\n",
    "        - annotations (pandas.DataFrame): Dataframe containing the annotations\n",
    "                                          for the joints.\n",
    "        - img_dir (pathlib.Path object or str): Path object or str specifying\n",
    "                                                the directory containing the\n",
    "                                                images.\n",
    "        - img_format (str: default=\"jpg\"): String specifying the format of the\n",
    "                                           images.\n",
    "        - transform (callable: default=None): Function or callable which returns\n",
    "                                              a transformed version of the image\n",
    "                                              passed as input.\n",
    "        - target_transform (callable: default=None): Function or callable  which\n",
    "                                                     returns a transformed\n",
    "                                                     version of the target label\n",
    "                                                     passed as input.\n",
    "        \"\"\"\n",
    "        self.annotations = annotations\n",
    "        self.img_dir = img_dir\n",
    "        self.img_format = img_format\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        pts = self.annotations.loc[index, slice(0, 13)]\n",
    "        img_num = self.annotations.loc[index, \"image_num\"]\n",
    "\n",
    "        # Append image extension and pad zeros in `img_num` to get the file name\n",
    "        file_name = f\"im{int(img_num)+1:04d}.{self.img_format}\"\n",
    "        img_file = os.path.join(self.img_dir, file_name)\n",
    "        img = torchvision.io.read_image(img_file)\n",
    "\n",
    "        # Store the original height & width of the images before transformation\n",
    "        # This will be used for scaling the coordinates of the target points\n",
    "        org_size = img.size()[-2:]\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "            new_size = self.transform.size\n",
    "\n",
    "            if self.target_transform:\n",
    "                # `org_size` & `new_size` are used to transform the coordinates\n",
    "                # of the joints\n",
    "                pts = self.target_transform(pts, org_size, new_size)\n",
    "\n",
    "        return img, pts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O9NGpNm_E-XW"
   },
   "outputs": [],
   "source": [
    "def transform_coords(pts, org_size, new_size):\n",
    "    \"\"\"Function to scale the coordinates in proportion to image resizing.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    - pts (pandas.DataFrame): DataFrame containing the points to be scaled.\n",
    "    - org_size (tuple or array-like): Original size of the image.\n",
    "    - new_size (tuple or array-like): Final size of the image after resizing.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    Numpy ndarray containing the scaled coordinates.\n",
    "    \"\"\"\n",
    "    new_x = pts.loc[:, \"x\"] * new_size[1] / org_size[1]\n",
    "    new_y = pts.loc[:, \"y\"] * new_size[0] / org_size[0]\n",
    "    new_pts = pd.concat([new_x, new_y], axis=1)\n",
    "\n",
    "    return torch.Tensor(new_pts.to_numpy().reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kHZnv8WkVnSt"
   },
   "outputs": [],
   "source": [
    "img_size = (3, 150, 125)\n",
    "# Define the mean & standard deviation to use for Normalizing the images\n",
    "img_mean = [0.485, 0.456, 0.406]\n",
    "img_std = [0.229, 0.224, 0.225]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eMD5cHDsYTAE"
   },
   "outputs": [],
   "source": [
    "transforms = {\n",
    "    \"train\": nn.Sequential(\n",
    "        T.Resize(img_size[1:]),\n",
    "        T.Normalize(img_mean, img_std),\n",
    "    ),\n",
    "    \"val\": nn.Sequential(\n",
    "        T.Resize(img_size[1:]),\n",
    "        T.Normalize(img_mean, img_std),\n",
    "    ),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2b6hy4jB-FPu"
   },
   "outputs": [],
   "source": [
    "datasets = {\n",
    "    phase: PoseDataset(\n",
    "        annotations=annotations[phase],\n",
    "        img_dir=root / \"images\",\n",
    "        transform=transforms[phase],\n",
    "        target_transform=transform_coords,\n",
    "    )\n",
    "    for phase in [\"train\", \"val\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FtJjNF-476_8"
   },
   "outputs": [],
   "source": [
    "len(datasets[\"train\"], datasets[\"val\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vNvtOMbj95RT"
   },
   "outputs": [],
   "source": [
    "# Define the batch size\n",
    "bs = 32\n",
    "n_workers = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cD_fHnc793Qe"
   },
   "outputs": [],
   "source": [
    "dataloaders = {\n",
    "    phase: DataLoader(\n",
    "        datasets[phase], batch_size=bs, shuffle=True, num_workers=n_workers\n",
    "    )\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VwHug9UyWwAf"
   },
   "source": [
    "## Visualizing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Lqwp-V3i4HmP"
   },
   "outputs": [],
   "source": [
    "joints = [\n",
    "    \"right ankle\",\n",
    "    \"right knee\",\n",
    "    \"right hip\",\n",
    "    \"left hip\",\n",
    "    \"left knee\",\n",
    "    \"left ankle\",\n",
    "    \"right wrist\",\n",
    "    \"right elbow\",\n",
    "    \"right shoulder\",\n",
    "    \"left shoulder\",\n",
    "    \"left elbow\",\n",
    "    \"left wrist\",\n",
    "    \"neck\",\n",
    "    \"head top\",\n",
    "]\n",
    "joints_map = {joint: i for i, joint in enumerate(joints)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1mrBX1pKQK38"
   },
   "outputs": [],
   "source": [
    "class COLOR:\n",
    "    \"\"\"Class containing variables for various colors.\n",
    "    Color scheme = (B,G,R)\n",
    "    \"\"\"\n",
    "\n",
    "    RED = (0, 0, 255)\n",
    "    GREEN = (0, 255, 0)\n",
    "    BLUE = (255, 0, 0)\n",
    "    CYAN = (255, 255, 0)\n",
    "    YELLOW = (0, 255, 255)\n",
    "    MAGENTA = (255, 0, 255)\n",
    "    LAVENDER = (250, 230, 230)\n",
    "    ORANGE = (0, 175, 255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1mrBX1pKQK38"
   },
   "outputs": [],
   "source": [
    "# List of joint-connections & colors being used for connecting the joints in\n",
    "# image annotations\n",
    "joints_relations = [\n",
    "    (\"right ankle\", \"right knee\", COLOR.CYAN),\n",
    "    (\"left ankle\", \"left knee\", COLOR.CYAN),\n",
    "    (\"right knee\", \"right hip\", COLOR.RED),\n",
    "    (\"left knee\", \"left hip\", COLOR.RED),\n",
    "    (\"right wrist\", \"right elbow\", COLOR.GREEN),\n",
    "    (\"left wrist\", \"left elbow\", COLOR.GREEN),\n",
    "    (\"right elbow\", \"right shoulder\", COLOR.BLUE),\n",
    "    (\"left elbow\", \"left shoulder\", COLOR.BLUE),\n",
    "    (\"neck\", \"head top\", COLOR.MAGENTA),\n",
    "    (\"neck\", \"hip line\", COLOR.YELLOW),  # hip line = mean of left & right hip\n",
    "    (\"right hip\", \"left hip\", COLOR.LAVENDER),\n",
    "    (\"right shoulder\", \"left shoulder\", COLOR.ORANGE),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3xSZ87lFeRNz"
   },
   "outputs": [],
   "source": [
    "def annotate_image(img, pts, mapping, relations):\n",
    "    \"\"\"Function to annotate an image with lines drawn between specified points.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    - img (torch.Tensor) - Image in (C,H,W) format. Color-scheme = (R,G,B).\n",
    "    - pts (array-like) - Points to use for annotating lines.\n",
    "    - mapping (dict-like) - Mapping from joint name to joint number.\n",
    "    - relations (list) - List containing tuples defining which points to join &\n",
    "                         with what color.\n",
    "    \"\"\"\n",
    "    # Change image format from (C, H, W) to (H, W, C)\n",
    "    img = img.clone().detach().permute((1, 2, 0))\n",
    "\n",
    "    # Change image from RBG to BGR (& change to np ndarray to show img)\n",
    "    img = torch.stack((img[:, :, 2], img[:, :, 1], img[:, :, 0]), dim=2).numpy()\n",
    "\n",
    "    # Calculate the coordinates for the `hip line` from coordinates of the\n",
    "    # `right hip` & the `left hip`\n",
    "    rhip = mapping.get(\"right hip\")\n",
    "    lhip = mapping.get(\"left hip\")\n",
    "    # Use floor divison to get integer coordinates (for annotating images)\n",
    "    hipline_coords = (pts[rhip] + pts[lhip]) // 2\n",
    "\n",
    "    for j1, j2, color in relations:\n",
    "        pt1 = mapping.get(j1, None)\n",
    "        pt2 = mapping.get(j2, None)\n",
    "        # Convert coordinates to `int` for annotating the image\n",
    "        # For \"hip line\" `pt1` or `pt2` will be equal `None` so we set it\n",
    "        # equal to the above calculated `hipline_coords`\n",
    "        pt1 = tuple(map(int, pts[pt1])) if pt1 is not None else hipline_coords\n",
    "        pt2 = tuple(map(int, pts[pt2])) if pt2 is not None else hipline_coords\n",
    "        # Annotate the image\n",
    "        cv2.line(img, pt1, pt2, color=color)\n",
    "    cv2_imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C1ZAR-JPjV4k"
   },
   "source": [
    "## Creating & Training a Model on the Data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LE_paPLZHh7X"
   },
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    \"\"\"Class implementing the model to be trained on the data.\"\"\"\n",
    "\n",
    "    def __init__(self, img_shape, output_shape):\n",
    "        super(Model, self).__init__()\n",
    "\n",
    "        self.img_shape = img_shape\n",
    "        self.output_shape = output_shape\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=img_shape[0], out_channels=8, kernel_size=3),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=1),\n",
    "            nn.Conv2d(in_channels=8, out_channels=16, kernel_size=3),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=1),\n",
    "            nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=1),\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=1),\n",
    "            nn.Conv2d(in_channels=64, out_channels=16, kernel_size=1),\n",
    "        )\n",
    "\n",
    "        self.flatten = nn.Flatten(start_dim=-3)\n",
    "\n",
    "        # Calculate the shape of the output produced by the convolution &\n",
    "        # pooling layers\n",
    "        out_shape = self._calc_output_shape()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(out_shape[0] * out_shape[1] * out_shape[2], 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, output_shape),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.conv(x)\n",
    "        y = self.flatten(y)\n",
    "        y = self.fc(y)\n",
    "        return y\n",
    "\n",
    "    def _calc_output_shape(self):\n",
    "        out_img_shape = list(self.img_shape)\n",
    "        for module in self.conv.modules():\n",
    "            if isinstance(module, (nn.Conv2d, nn.MaxPool2d, nn.AvgPool2d)):\n",
    "                # Kernel size\n",
    "                k = module.kernel_size\n",
    "                k = k if isinstance(k, tuple) else (k, k)\n",
    "                # Stride\n",
    "                s = module.stride\n",
    "                s = s if isinstance(s, tuple) else (s, s)\n",
    "                # Padding\n",
    "                p = module.padding\n",
    "                p = p if isinstance(p, tuple) else (p, p)\n",
    "                # Dilation\n",
    "                d = module.dilation\n",
    "                d = d if isinstance(d, tuple) else (d, d)\n",
    "                # Update no. of channels (for Conv2d layers)\n",
    "                if hasattr(module, \"out_channels\"):\n",
    "                    out_img_shape[0] = module.out_channels\n",
    "                # Update image height\n",
    "                out_img_shape[1] = (\n",
    "                    out_img_shape[1] + 2 * p[0] - d[0] * (k[0] - 1) - 1\n",
    "                ) // s[0] + 1\n",
    "                # Update image width\n",
    "                out_img_shape[2] = (\n",
    "                    out_img_shape[2] + 2 * p[1] - d[1] * (k[1] - 1) - 1\n",
    "                ) // s[1] + 1\n",
    "\n",
    "        return out_img_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val_model(\n",
    "    model, dataloaders, loss_fn, optimizer, num_epochs, lr_scheduler=None, device=\"cpu\"\n",
    "):\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Initialize the best model weights as the initial weights of the model\n",
    "    best_model_parameters = copy.deepcopy(model.state_dict())\n",
    "    min_val_loss = float(\"inf\")\n",
    "\n",
    "    # Move the `model` to the specified `device`\n",
    "    model = model.to(device)\n",
    "\n",
    "    # Initially set the gradients to zero\n",
    "    optimizer.grad_zero()\n",
    "\n",
    "    for i in range(num_epochs):\n",
    "        print(f\"Epoch {i}/{num_epochs-1}\")\n",
    "        print(\"-\" * 15)\n",
    "\n",
    "        # Both the training & validation datasets are passed to the model in\n",
    "        # each epoch\n",
    "        for phase in [\"train\", \"val\"]:\n",
    "            # Set mode depending upon the phase\n",
    "            if phase == \"train\":\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    "\n",
    "            total_loss = 0.0\n",
    "\n",
    "            with torch.set_grad_enabled(phase == \"train\"):\n",
    "                for imgs, targets in dataloaders[\"phase\"]:\n",
    "                    imgs = imgs.to(device)\n",
    "                    targets = targets.to(device)\n",
    "                    predictions = model(imgs)\n",
    "                    loss = loss_fn(predictions, targets)\n",
    "                    total_loss += loss.item() * imgs.size(0)\n",
    "\n",
    "                    # Update parameter values during the training phase\n",
    "                    if phase == \"train\":\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                        # Set the gradients to zero\n",
    "                        optimizer.grad_zero()\n",
    "\n",
    "            # Update the learning rate using the `lr_scheduler`\n",
    "            # Only count training loops for updating parameters\n",
    "            if lr_scheduler and phase == \"train\":\n",
    "                lr_scheduler.step()\n",
    "            avg_loss = total_loss / len(datasets[phase])\n",
    "            print(f\"{phase.capitalize()} Average Loss: {avg_loss: .4f}\")\n",
    "\n",
    "            if phase == \"val\" and avg_loss < min_val_loss:\n",
    "                best_model_parameters = copy.deepcopy(model.state_dict())\n",
    "                min_val_loss = avg_loss\n",
    "        # Print a new line after each epoch\n",
    "        print()\n",
    "\n",
    "        total_time = time.time() - start_time\n",
    "        print(f\"Model training completed in {total_time//60}m {total_time % 60}s\")\n",
    "        print(f\"Minimum average validation loss: {min_val_loss}\")\n",
    "\n",
    "        return best_model_parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A0sEIybwtQre"
   },
   "source": [
    "##### Use `GPU` for training if available else use `CPU`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3cwG_3Iztq58"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device being used: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j7MIRmuhOl6d"
   },
   "outputs": [],
   "source": [
    "from torch.optim import Adam"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "HumanPoseDetection.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
