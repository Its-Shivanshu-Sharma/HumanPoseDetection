{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HumanPoseDetection.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Its-Shivanshu-Sharma/HumanPoseDetection/blob/main/HumanPoseDetection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installing Libraries"
      ],
      "metadata": {
        "id": "J5H1r-t0n8et"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "pip3 install -qq torch==1.10.2+cu113 torchvision==0.11.3+cu113 -f https://download.pytorch.org/whl/cu113/torch_stable.html"
      ],
      "metadata": {
        "collapsed": true,
        "id": "_U4BlQltnrGN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader"
      ],
      "metadata": {
        "id": "kbTjTbjZoLBp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fetching & Preparing the Dataset"
      ],
      "metadata": {
        "id": "aRSMwWYxoxoC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the seed for the RNG(Random Number Generator manually\n",
        "# (for reproducibility)\n",
        "torch.manual_seed(10)"
      ],
      "metadata": {
        "id": "4bjdiMK4z9nn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### NOTE:\n",
        "The dataset used in this project is the [`Leeds Sports Pose Dataset`](http://sam.johnson.io/research/lsp.html)."
      ],
      "metadata": {
        "id": "ZMxLRWJm2fDn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "# Create a new directory called `pose_dataset` to store the data\n",
        "mkdir pose_dataset\n",
        "cd pose_dataset\n",
        "wget -q http://sam.johnson.io/research/lsp_dataset.zip\n",
        "unzip -qq lsp_dataset.zip\n",
        "rm lsp_dataset.zip"
      ],
      "metadata": {
        "id": "u5hNxqfCHsSR",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "from scipy.io import loadmat"
      ],
      "metadata": {
        "id": "8AYb25YfpAdd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "root = Path(\"./pose_dataset\")"
      ],
      "metadata": {
        "id": "38Tc_eQzpkmV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_data = loadmat(root/\"joints.mat\")"
      ],
      "metadata": {
        "id": "J5094mpzKb6l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_data_x = raw_data[\"joints\"][0].T.reshape(-1, 1).squeeze()\n",
        "raw_data_y = raw_data[\"joints\"][1].T.reshape(-1, 1).squeeze()"
      ],
      "metadata": {
        "id": "3uYwwN7_BPZL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_data_x.shape"
      ],
      "metadata": {
        "id": "A37N2CTGuwOt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_joints = 14\n",
        "index = pd.MultiIndex.from_product((range(len(raw_data_x)//n_joints), range(n_joints)), names=[\"image_num\", \"joint\"])\n",
        "cols = pd.Index([\"x\", \"y\"], name=\"coordinates\")"
      ],
      "metadata": {
        "id": "LrB9eHWHY_As"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.DataFrame(zip(raw_data_x, raw_data_y), index=index, columns=cols)\n",
        "data"
      ],
      "metadata": {
        "id": "c5tDYfvfKr-C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = data.unstack(1).swaplevel(i=0, j=1, axis=1).sort_index(axis=1)\n",
        "data"
      ],
      "metadata": {
        "id": "ANBWo9M5lA-F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_val_split(data, train_size=None, val_size=None):\n",
        "    \"\"\"Function for randomly data into training & validation sets.\n",
        "    Splits the data in training and validation sets as per the size specified by\n",
        "    the parmaters `train_size` or `val_size`.\n",
        "    Indexes of training & validation sets are reset to 0. The old indexes are\n",
        "    stored in a new column named `image_num` as they are used to refer to the\n",
        "    images.\n",
        "\n",
        "    Parameters:\n",
        "        data (pandas.DataFrame): dataframe containing the data(i.e. annotations)\n",
        "        train_size (float): fraction of the data to be allocated to training set\n",
        "        val_size (float): fraction of the data to be allocated to validation\n",
        "                            set. Ignored if `train_size` is not None.\n",
        "        \n",
        "    \"\"\"\n",
        "    data_len = len(data)\n",
        "    if train_size:\n",
        "        train_len = int(data_len*train_size)\n",
        "    elif val_size:\n",
        "        train_len = data_len - int(data_len*val_size)\n",
        "\n",
        "    rand_idxs = torch.randperm(data_len)\n",
        "    train_set = data.loc[rand_idxs[:train_len]]\n",
        "    val_set = data.loc[rand_idxs[train_len:]]\n",
        "\n",
        "    # Reset indices of `train_set` & `val_set` to start from 0\n",
        "    train_set = train_set.reset_index().sort_index(axis=1)\n",
        "    val_set = val_set.reset_index().sort_index(axis=1)\n",
        "\n",
        "    return train_set, val_set"
      ],
      "metadata": {
        "id": "dYG2cb1saVdx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_set, val_set = train_val_split(data, train_size=0.8)\n",
        "train_set.shape, val_set.shape"
      ],
      "metadata": {
        "id": "Hhr7sisgb84L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating `Dataset` & `DataLoader` objects"
      ],
      "metadata": {
        "id": "EXroCmDXk97k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from torchvision.io import read_image"
      ],
      "metadata": {
        "id": "k5D_2HlUlDhx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PoseDataset(Dataset):\n",
        "    def __init__(self, annotations, img_dir, img_format=\"jpg\", transform=None, target_transform=None):\n",
        "        self.annotations = annotations\n",
        "        self.img_dir = img_dir\n",
        "        self.img_format = img_format\n",
        "        self.transform = transform\n",
        "        self.target_transform = target_transform\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.annotations)\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        pts = self.annotations.loc[index, slice(0, 13)]\n",
        "        img_num = self.annotations.loc[index, \"image_num\"]\n",
        "        file_name = f\"im{int(img_num)+1:04d}.{self.img_format}\"\n",
        "        img_file = os.path.join(self.img_dir, file_name)\n",
        "        img = read_image(img_file)\n",
        "\n",
        "        org_size = img.size()[-2:]\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "            new_size = self.transform.size\n",
        "        \n",
        "            if self.target_transform:\n",
        "                # `org_size` & `new_size` are used to transform the coordinates\n",
        "                # of the joints\n",
        "                pts = self.target_transform(pts, org_size, new_size)\n",
        "        \n",
        "        return img, pts"
      ],
      "metadata": {
        "id": "bcHHTaDftak0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.transforms import Resize"
      ],
      "metadata": {
        "id": "jXLFo30-99GH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def transform_coords(pts, org_size, new_size, reshape=True):\n",
        "    \"\"\"Function to scale the coordinates in proportion to image resizing.\n",
        "    Parameters:\n",
        "    pts (pandas.DataFrame) - DataFrame containing the points to be scaled.\n",
        "    org_size (tuple or array-like) - Original size of the image.\n",
        "    new_size (tuple or array-like) - Final size of the image after resizing.\n",
        "    reshape (bool) - Boolean denoting whether to finally return points should be \n",
        "                        a numpy 1d-ndarray or a pandas dataframe.\n",
        "    \"\"\"\n",
        "    new_x = pts.loc[:, \"x\"]*new_size[1]/org_size[1]\n",
        "    new_y = pts.loc[:, \"y\"]*new_size[0]/org_size[0]\n",
        "    new_pts = pd.concat([new_x, new_y], axis=1)\n",
        "\n",
        "    if reshape:\n",
        "        return new_pts.to_numpy().reshape(-1, 1).squeeze()"
      ],
      "metadata": {
        "id": "O9NGpNm_E-XW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img_size = (3, 150, 125)"
      ],
      "metadata": {
        "id": "kHZnv8WkVnSt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = PoseDataset(annotations=train_set,\n",
        "                      img_dir=root/\"images\",\n",
        "                      transform=Resize(img_size[1:]),\n",
        "                      target_transform=transform_coords)\n",
        "val_data = PoseDataset(annotations=val_set,\n",
        "                      img_dir=root/\"images\",\n",
        "                      transform=Resize(img_size[1:]),\n",
        "                      target_transform=transform_coords)"
      ],
      "metadata": {
        "id": "2b6hy4jB-FPu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_data), len(val_data)"
      ],
      "metadata": {
        "id": "FtJjNF-476_8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the batch size\n",
        "bs = 32"
      ],
      "metadata": {
        "id": "vNvtOMbj95RT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataloader = DataLoader(train_data, batch_size=bs, shuffle=True)\n",
        "val_dataloader = DataLoader(val_data, batch_size=bs, shuffle=True)"
      ],
      "metadata": {
        "id": "cD_fHnc793Qe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualizing the Data"
      ],
      "metadata": {
        "id": "VwHug9UyWwAf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab.patches import cv2_imshow\n",
        "import cv2"
      ],
      "metadata": {
        "id": "aOO5PjEIxZMe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "joints = [\n",
        "    \"right ankle\",\n",
        "    \"right knee\",\n",
        "    \"right hip\",\n",
        "    \"left hip\",\n",
        "    \"left knee\",\n",
        "    \"left ankle\",\n",
        "    \"right wrist\",\n",
        "    \"right elbow\",\n",
        "    \"right shoulder\",\n",
        "    \"left shoulder\",\n",
        "    \"left elbow\",\n",
        "    \"left wrist\",\n",
        "    \"neck\",\n",
        "    \"head top\",\n",
        "]\n",
        "joints_map = {joint:i for i, joint in enumerate(joints)}"
      ],
      "metadata": {
        "id": "Lqwp-V3i4HmP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class COLOR:\n",
        "    \"\"\"Class containing variables for various colors.\n",
        "    Color scheme = (B,G,R)\n",
        "    \"\"\"\n",
        "    RED = (0, 0, 255)\n",
        "    GREEN = (0, 255, 0)\n",
        "    BLUE = (255, 0, 0)\n",
        "    CYAN = (255, 255, 0)\n",
        "    YELLOW = (0, 255, 255)\n",
        "    MAGENTA = (255, 0, 255)\n",
        "    LAVENDER = (250, 230, 230)\n",
        "    ORANGE = (0, 175, 255)\n",
        "\n",
        "# List containing which joints must be connected using a line\n",
        "# and with what color\n",
        "joints_relations = [\n",
        "    (\"right ankle\", \"right knee\", COLOR.CYAN),\n",
        "    (\"left ankle\", \"left knee\", COLOR.CYAN),\n",
        "    (\"right knee\", \"right hip\", COLOR.RED),\n",
        "    (\"left knee\", \"left hip\", COLOR.RED),\n",
        "    (\"right wrist\", \"right elbow\", COLOR.GREEN),\n",
        "    (\"left wrist\", \"left elbow\", COLOR.GREEN),\n",
        "    (\"right elbow\", \"right shoulder\", COLOR.BLUE),\n",
        "    (\"left elbow\", \"left shoulder\", COLOR.BLUE),\n",
        "    (\"neck\", \"head top\", COLOR.MAGENTA),\n",
        "    (\"neck\", \"hip line\", COLOR.YELLOW), # hip line = mean of left & right hip\n",
        "    (\"right hip\", \"left hip\", COLOR.LAVENDER),\n",
        "    (\"right shoulder\", \"left shoulder\", COLOR.ORANGE)\n",
        "]\n"
      ],
      "metadata": {
        "id": "1mrBX1pKQK38"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def annotate_image(img, pts, mapping, relations):\n",
        "    \"\"\"Function to annotate an image with lines drawn between specified points.\n",
        "    Parameters:\n",
        "    img (array-like) - Image in (C,H,W) format. Color-scheme = (R,G,B)\n",
        "    pts (array-like) - Points to be use for annotating lines.\n",
        "    mapping (dict) - Dict used to get coordinates for points used in `relation` \n",
        "                        from `pts`\n",
        "    relations (list) - List containing tuples defining which points to join & \n",
        "                        with what color.\n",
        "    \"\"\"\n",
        "    # Change image format from (C, H, W) to (H, W, C) \n",
        "    img = img.clone().detach().permute((1, 2, 0))\n",
        "    # Change image from RBG to BGR (& change to np ndarray to show img)\n",
        "    img = torch.stack((img[:, :, 2], img[:, :, 1], img[:, :, 0]), dim=2).numpy()\n",
        "    # Calculate the coordinates for the hip from right & left hip\n",
        "    rhip, lhip = mapping.get(\"right hip\"), mapping.get(\"left hip\")\n",
        "    hipline_coords = tuple(map(int, (pts[rhip]+pts[lhip])/2))\n",
        "\n",
        "    for j1, j2, color in relations:\n",
        "        pt1 = mapping.get(j1, None)\n",
        "        pt2 = mapping.get(j2, None)\n",
        "        # Convert coordinates to `int` for annotating the image\n",
        "        # for \"hip line\" `pt1` or `pt2` will be equal `None` so we set it \n",
        "        # equal to the above calculated `hipline_coords`\n",
        "        pt1 = tuple(map(int, pts[pt1])) if pt1 is not None else hipline_coords\n",
        "        pt2 = tuple(map(int, pts[pt2])) if pt2 is not None else hipline_coords\n",
        "        # Annotate the image\n",
        "        cv2.line(img, pt1, pt2, color=color)\n",
        "    cv2_imshow(img)"
      ],
      "metadata": {
        "id": "3xSZ87lFeRNz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating & Training a Model on the Data."
      ],
      "metadata": {
        "id": "C1ZAR-JPjV4k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Model(nn.Module):\n",
        "    \"\"\"Class implementing the model to be trained on the data.\"\"\"\n",
        "    def __init__(self, img_shape, output_shape):\n",
        "        super(Model, self).__init__()\n",
        "\n",
        "        self.img_shape = img_shape\n",
        "        self.output_shape = output_shape\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=img_shape[0],\n",
        "                      out_channels=8,\n",
        "                      kernel_size=3),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=1),\n",
        "            nn.Conv2d(in_channels=8,\n",
        "                      out_channels=16,\n",
        "                      kernel_size=3),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=1),\n",
        "            nn.Conv2d(in_channels=16,\n",
        "                      out_channels=32,\n",
        "                      kernel_size=3),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=1),\n",
        "            nn.Conv2d(in_channels=32,\n",
        "                      out_channels=64,\n",
        "                      kernel_size=3),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=1),\n",
        "            nn.Conv2d(in_channels=64,\n",
        "                      out_channels=16,\n",
        "                      kernel_size=1),\n",
        "        )\n",
        "\n",
        "        self.flatten = nn.Flatten(start_dim=-3)\n",
        "\n",
        "        # Calculate the shape of the output produced by the convolution & \n",
        "        # pooling layers\n",
        "        out_shape = self._calc_output_shape()\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(out_shape[0]*out_shape[1]*out_shape[2], 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 32),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(32, output_shape),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "    \n",
        "    def forward(self, x):\n",
        "        y = self.conv(x)\n",
        "        y = self.flatten(y)\n",
        "        y = self.fc(y)\n",
        "        return y\n",
        "\n",
        "    def _calc_output_shape(self):\n",
        "        out_img_shape = list(self.img_shape)\n",
        "        for module in self.conv.modules():\n",
        "            if isinstance(module, (nn.Conv2d, nn.MaxPool2d, nn.AvgPool2d)):\n",
        "                # Kernel size\n",
        "                k = module.kernel_size\n",
        "                k = k if isinstance(k, tuple) else (k, k)\n",
        "                # Stride\n",
        "                s = module.stride\n",
        "                s = s if isinstance(s, tuple) else (s, s)\n",
        "                # Padding\n",
        "                p = module.padding\n",
        "                p = p if isinstance(p, tuple) else (p, p)\n",
        "                # Dilation\n",
        "                d = module.dilation\n",
        "                d = d if isinstance(d, tuple) else (d, d)\n",
        "                # Update no. of channels (for Conv2d layers)\n",
        "                if hasattr(module, \"out_channels\"):\n",
        "                    out_img_shape[0] = module.out_channels\n",
        "                # Update image height\n",
        "                out_img_shape[1] = (out_img_shape[1]+2*p[0]-d[0]*(k[0]-1)-1)//s[0] + 1\n",
        "                # Update image width\n",
        "                out_img_shape[2] = (out_img_shape[2]+2*p[1]-d[1]*(k[1]-1)-1)//s[1] + 1\n",
        "                \n",
        "        return out_img_shape"
      ],
      "metadata": {
        "id": "LE_paPLZHh7X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_loop(train_dataloader, model, loss_fn, optimizer, device=\"cpu\"):\n",
        "    \"\"\"Function to perform the training on the data for one epoch\n",
        "    Parameters:\n",
        "    train_dataloader (DataLoader) - dataloader object for the training data\n",
        "    model - the which has to be trained (& used for making predictions)\n",
        "    loss_fn (function) - loss function to use to calculate gradients\n",
        "    optimizer - optimizer to use to update the parameters of the model\n",
        "    device (str: default=\"cpu\") - device to use for training\n",
        "    \"\"\"\n",
        "    model.train()\n",
        "    avg_loss = 0\n",
        "    for x, y in train_dataloader:\n",
        "        x = x.float().to(device)\n",
        "        y = y.float().to(device)\n",
        "        optimizer.zero_grad()\n",
        "        pred = model(x)\n",
        "        # Calculate loss & gradients\n",
        "        loss = loss_fn(pred, y)\n",
        "        avg_loss += loss.item()\n",
        "        loss.backward()\n",
        "        # Update parameter values\n",
        "        optimizer.step()\n",
        "    print(f\"avg. training loss: {avg_loss/len(train_dataloader)}\")"
      ],
      "metadata": {
        "id": "J2ZX_XNm_6_B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def val_loop(val_dataloader, model, loss_fn, device=\"cpu\"):\n",
        "    \"\"\"Function to calculate the loss & accuracy of the model for the validation\n",
        "    dataset.\n",
        "    Parameters:\n",
        "    val_dataloader (DataLoader) - dataloader object for the validation data\n",
        "    model - the which has to be trained (& used for making predictions)\n",
        "    loss_fn (function) - loss function to use to calculate gradients\n",
        "    device (str: default=\"cpu\") - device to use for inference\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    avg_loss = 0\n",
        "    # Gradient need not be computed for the validation data, hence, computations\n",
        "    # can be sped up\n",
        "    with torch.no_grad():\n",
        "        for x, y in val_dataloader:\n",
        "            x = x.float().to(device)\n",
        "            y = y.float().to(device)\n",
        "            pred = model(x)\n",
        "            avg_loss += loss_fn(pred, y).item()\n",
        "    print(f\"avg. validation loss: {avg_loss/len(val_dataloader)}\")"
      ],
      "metadata": {
        "id": "s_iGTBa1Bix3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Use `GPU` for training if available else use `CPU`"
      ],
      "metadata": {
        "id": "A0sEIybwtQre"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Device being used: {device}\")"
      ],
      "metadata": {
        "id": "3cwG_3Iztq58"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Model(img_size, n_joints*2)\n",
        "model.to(device);"
      ],
      "metadata": {
        "id": "BEv13zkFt7OB",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.optim import Adam"
      ],
      "metadata": {
        "id": "j7MIRmuhOl6d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_epochs = 15\n",
        "lr = 1e-3\n",
        "optimizer = Adam(model.parameters(), lr)\n",
        "mse_loss = nn.MSELoss()"
      ],
      "metadata": {
        "id": "12Yinrb_OrWJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(n_epochs):\n",
        "    line_len = 50\n",
        "    print(\"_\"*line_len)\n",
        "    print(f\"Epoch No. {i}\")\n",
        "    print(\"_\"*line_len)\n",
        "    train_loop(train_dataloader, model, mse_loss, optimizer, device)\n",
        "    val_loop(val_dataloader, model, mse_loss, device)\n",
        "    print(\"_\"*50)"
      ],
      "metadata": {
        "id": "kewGb2JbsOvr",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    img, pts = val_data[4]\n",
        "    preds = model(img.unsqueeze(0).float().to(device))\n",
        "    annotate_image(img, preds.reshape(-1, 2), joints_map, joints_relations)"
      ],
      "metadata": {
        "id": "qbN9Gfd4WywW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "EvhOhcKYWtFr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}